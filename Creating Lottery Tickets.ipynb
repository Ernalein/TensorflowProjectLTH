{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "53a90692",
   "metadata": {},
   "source": [
    "# Creating Winning Tickets\n",
    "\n",
    "In this notebook we will Create Winning Tickets. To do so we safe an initial weight configuration of a CNN and compare the training performance of the configuration for basic training and training with most of the weights pruned using iterative magnitude pruning with resetting."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33f4cfa5",
   "metadata": {},
   "source": [
    "## A: importing libraries, data preprocessing and the CNN architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "47cc9e09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import necessary libraries\n",
    "\n",
    "import tensorflow_datasets as tfds\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3361321e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# download dataset and preprocess dataset\n",
    "\n",
    "def load_and_prep_cifar(batch_size, shuffle_size):\n",
    "    # load data set\n",
    "    (train_ds, test_ds), ds_info = tfds.load(name=\"cifar10\", split=[\"train\",\"test\"], as_supervised=True, with_info=True)\n",
    "    # tfds.show_examples(train_ds, ds_info)\n",
    "    \n",
    "    def prepare_cifar10_data(ds):\n",
    "        #convert data from uint8 to float32\n",
    "        ds = ds.map(lambda img, target: (tf.cast(img, tf.float32), target))\n",
    "        #sloppy input normalization, just bringing image values from range [0, 255] to [-1, 1]\n",
    "        ds = ds.map(lambda img, target: ((img/128.)-1., target))\n",
    "        #create one-hot targets\n",
    "        ds = ds.map(lambda img, target: (img, tf.one_hot(target, depth=10)))\n",
    "        #cache this progress in memory, as there is no need to redo it; it is deterministic after all\n",
    "        ds = ds.cache()\n",
    "        #shuffle, batch, prefetch\n",
    "        ds = ds.shuffle(shuffle_size).batch(batch_size).prefetch(2)\n",
    "        #return preprocessed dataset\n",
    "        return ds\n",
    "    \n",
    "    # prepare data\n",
    "    train_dataset = train_ds.apply(prepare_cifar10_data)\n",
    "    test_dataset = test_ds.apply(prepare_cifar10_data)\n",
    "    \n",
    "    return train_dataset, test_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "35c8a0fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the model\n",
    "\n",
    "class CNN2Model(tf.keras.Model):\n",
    "    \n",
    "    # basic\n",
    "    def __init__(self):\n",
    "        super(CNN2Model, self).__init__()\n",
    "        \n",
    "        # set biases to a value that is not exactly 0.0, so they don't get handled like pruned values\n",
    "        self.bias_in = tf.keras.initializers.Constant(value=0.0000000001)\n",
    "        \n",
    "        self.conv1 = tf.keras.layers.Conv2D(filters=64, kernel_size=3,activation=\"relu\", padding=\"same\",kernel_initializer='glorot_uniform', bias_initializer=self.bias_in) # [batchsize,32,32,64]\n",
    "        self.conv2 = tf.keras.layers.Conv2D(filters=64, kernel_size=3,activation=\"relu\", padding=\"same\",kernel_initializer='glorot_uniform', bias_initializer=self.bias_in) # [batchsize,32,32,64]\n",
    "        self.maxpool = tf.keras.layers.MaxPooling2D(pool_size=(2, 2),strides=(2, 2),input_shape=(32, 32, 64)) # [batchsize,16,16,64]\n",
    "        self.flatten = tf.keras.layers.Flatten() # [batch_size,16384]\n",
    "        self.dense1 = tf.keras.layers.Dense(256, activation=\"relu\",kernel_initializer='glorot_uniform', bias_initializer=self.bias_in) # [batch_size,256]\n",
    "        self.dense2 = tf.keras.layers.Dense(256, activation=\"relu\",kernel_initializer='glorot_uniform', bias_initializer=self.bias_in) # [batch_size,256]\n",
    "        self.dense3 = tf.keras.layers.Dense(10, activation=\"softmax\",kernel_initializer='glorot_uniform', bias_initializer=self.bias_in) # [batch_size,256]\n",
    "\n",
    "    @tf.function\n",
    "    def call(self, inputs):\n",
    "        x = self.conv1(inputs)\n",
    "        x = self.conv2(x)\n",
    "        x = self.maxpool(x)\n",
    "        x = self.flatten(x)\n",
    "        x = self.dense1(x)\n",
    "        x = self.dense2(x)\n",
    "        x = self.dense3(x)\n",
    "        return x\n",
    "            \n",
    "    \n",
    "    def get_conv_weights(self):\n",
    "        return self.conv1.get_weights(), self.conv2.get_weights(), self.dense3.get_weights()\n",
    "        \n",
    "    def set_conv_weights(self,weights_conv1, weights_conv2, weights_dense3):\n",
    "        self.conv1.set_weights(weights_conv1)\n",
    "        self.conv2.set_weights(weights_conv2)\n",
    "        self.dense3.set_weights(weights_dense3)\n",
    "        \n",
    "    def get_dense_weights(self):\n",
    "        return self.dense1.get_weights(), self.dense2.get_weights()\n",
    "        \n",
    "    def set_dense_weights(self,weights_dense1, weights_dense2):\n",
    "        self.dense1.set_weights(weights_dense1)\n",
    "        self.dense2.set_weights(weights_dense2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fdab76e",
   "metadata": {},
   "source": [
    "## B: Three types of Training loops\n",
    "\n",
    "One Trianing Loop for basic training, one for iterative magnitude pruning with resetting and one for basic training of a sparse network.  \n",
    "For iterative magnitude pruning with resetting after x epochs of trianing the p weights with the smallest magnitudes are set to zero. p depends on the layer. weights with a value of zero are then excluded from further optimization. All other weights are resetted to their initial value and then this process is repeated several times.  \n",
    "Taining of a sparse network is similar to the basic training with the difference that weights with a value of 0 are excluded from training.\n",
    "There is also a plotting funtion to show loss and accuracy for training and testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "84084836",
   "metadata": {},
   "outputs": [],
   "source": [
    "# basic train loop\n",
    "\n",
    "def train_loop(train, test, model, num_epochs):\n",
    "    \n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=0.0002)\n",
    "    loss_function= tf.keras.losses.CategoricalCrossentropy()\n",
    "    \n",
    "    train_accuracy = tf.keras.metrics.Accuracy(name='test_accuracy')\n",
    "    test_accuracy = tf.keras.metrics.Accuracy(name='train_accuracy')\n",
    "    train_losses = tf.keras.metrics.CategoricalCrossentropy(name='train_losses')\n",
    "    test_losses = tf.keras.metrics.CategoricalCrossentropy(name='test_losses')\n",
    "    train_acc = []\n",
    "    test_acc = []\n",
    "    train_l =[]\n",
    "    test_l = []\n",
    "\n",
    "    for e in tqdm(range(num_epochs)):\n",
    "        \n",
    "        #train step\n",
    "        for x, t in train:\n",
    "            with tf.GradientTape() as tape:\n",
    "                pred = model(x)\n",
    "                loss = loss_function(t, pred)\n",
    "                train_losses.update_state(t, pred)\n",
    "                train_accuracy.update_state(tf.argmax(t,1), tf.argmax(pred,1))\n",
    "            gradients = tape.gradient(loss, model.trainable_variables)\n",
    "            optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "            \n",
    "        # test step\n",
    "        for x, t in test:\n",
    "            pred = model(x)\n",
    "            test_accuracy.update_state(tf.argmax(t,1), tf.argmax(pred,1))\n",
    "            test_losses.update_state(t, pred)\n",
    "        \n",
    "        train_acc.append(train_accuracy.result().numpy())\n",
    "        test_acc.append(test_accuracy.result().numpy())\n",
    "        train_l.append(train_losses.result().numpy())\n",
    "        test_l.append(test_losses.result().numpy())\n",
    "        train_accuracy.reset_state()\n",
    "        test_accuracy.reset_state()\n",
    "        train_losses.reset_state()\n",
    "        test_losses.reset_state()     \n",
    "    \n",
    "    return  train_acc, test_acc, train_l, test_l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "225171e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train loop of iterative magnitude pruning\n",
    "\n",
    "def train_loop_imp(train, test, model, pruning_rounds=11, num_epochs_per_pruning=8, pruning_rate_conv=10, pruning_rate_dense=20):\n",
    "    \n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=0.0002)\n",
    "    loss_function= tf.keras.losses.CategoricalCrossentropy()\n",
    "    \n",
    "    train_accuracy = tf.keras.metrics.Accuracy(name='test_accuracy')\n",
    "    test_accuracy = tf.keras.metrics.Accuracy(name='train_accuracy')\n",
    "    train_losses = tf.keras.metrics.CategoricalCrossentropy(name='train_losses')\n",
    "    test_losses = tf.keras.metrics.CategoricalCrossentropy(name='test_losses')\n",
    "    train_acc = []\n",
    "    test_acc = []\n",
    "    train_l =[]\n",
    "    test_l = []\n",
    "    \n",
    "    # safe initial weights for conv layers (and last layer, because they share the same pruning rate)\n",
    "    conv_initial_weights = []\n",
    "    for weight_matrix in model.get_conv_weights():\n",
    "        conv_initial_weights.append(weight_matrix)\n",
    "        \n",
    "    # safe initial weights for dense layers\n",
    "    dense_initial_weights = []\n",
    "    for weight_matrix in model.get_dense_weights():\n",
    "        dense_initial_weights.append(weight_matrix)\n",
    "        \n",
    "        \n",
    "    for r in tqdm(range(pruning_rounds)):\n",
    "    \n",
    "        for e in tqdm(range(num_epochs_per_pruning)):\n",
    "\n",
    "            #train step\n",
    "            for x, t in train:\n",
    "                with tf.GradientTape() as tape:\n",
    "                    pred = model(x)\n",
    "                    loss = loss_function(t, pred)\n",
    "                    train_losses.update_state(t, pred)\n",
    "                    train_accuracy.update_state(tf.argmax(t,1), tf.argmax(pred,1))\n",
    "                gradients = tape.gradient(loss, model.trainable_variables)\n",
    "                # set gradients to zero for all pruned weights\n",
    "                for gradient_matrix, variables in zip(gradients, model.trainable_variables):\n",
    "                    gradient_matrix = np.where(variables == 0.0, 0.0, gradient_matrix)\n",
    "                optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "\n",
    "            # test step\n",
    "            for x, t in test:\n",
    "                pred = model(x)\n",
    "                test_accuracy.update_state(tf.argmax(t,1), tf.argmax(pred,1))\n",
    "                test_losses.update_state(t, pred)\n",
    "\n",
    "            train_acc.append(train_accuracy.result().numpy())\n",
    "            test_acc.append(test_accuracy.result().numpy())\n",
    "            train_l.append(train_losses.result().numpy())\n",
    "            test_l.append(test_losses.result().numpy())\n",
    "            train_accuracy.reset_state()\n",
    "            test_accuracy.reset_state()\n",
    "            train_losses.reset_state()\n",
    "            test_losses.reset_state()\n",
    "\n",
    "        # at end of each round, prune the smallest weights(not including biases) by setting them to zero \n",
    "        # and reset all other weights to their initial value   \n",
    "        \n",
    "        new_conv_weights = []   \n",
    "        for weight_matrix, init_weights in zip(model.get_conv_weights(), conv_initial_weights):\n",
    "            # calculate the percentile\n",
    "            percentile = np.percentile(np.abs(weight_matrix[0][weight_matrix[0] != 0.0].flatten()), pruning_rate_conv,)\n",
    "            # set pruned weights to zero and reset other weights\n",
    "            new_conv_weights.append([np.where(weight_matrix[0] < percentile, 0, init_weights[0]), init_weights[1]])\n",
    "        model.set_conv_weights(new_conv_weights[0], new_conv_weights[1], new_conv_weights[2])\n",
    "        \n",
    "        new_dense_weights = []\n",
    "        for weight_matrix, init_weights in zip(model.get_dense_weights(), dense_initial_weights):\n",
    "            # calculate the percentile\n",
    "            percentile = np.percentile(np.abs(weight_matrix[0][weight_matrix[0] != 0.0].flatten()), pruning_rate_dense,)\n",
    "            # set pruned weights to zero  and reset other weights\n",
    "            new_dense_weights.append([np.where(weight_matrix[0] < percentile, 0, init_weights[0]), init_weights[1]])\n",
    "        model.set_dense_weights(new_dense_weights[0], new_dense_weights[1])\n",
    "            \n",
    "    \n",
    "    return  train_acc, test_acc, train_l, test_l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "272b86bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train loop for sparse networks\n",
    "\n",
    "def train_loop_sparse(train, test, model, num_epochs):\n",
    "    \n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=0.0002)\n",
    "    loss_function= tf.keras.losses.CategoricalCrossentropy()\n",
    "    \n",
    "    train_accuracy = tf.keras.metrics.Accuracy(name='test_accuracy')\n",
    "    test_accuracy = tf.keras.metrics.Accuracy(name='train_accuracy')\n",
    "    train_losses = tf.keras.metrics.CategoricalCrossentropy(name='train_losses')\n",
    "    test_losses = tf.keras.metrics.CategoricalCrossentropy(name='test_losses')\n",
    "    train_acc = []\n",
    "    test_acc = []\n",
    "    train_l =[]\n",
    "    test_l = []\n",
    "\n",
    "    for e in tqdm(range(num_epochs)):\n",
    "        \n",
    "        #train step\n",
    "        for x, t in train:\n",
    "            with tf.GradientTape() as tape:\n",
    "                pred = model(x)\n",
    "                loss = loss_function(t, pred)\n",
    "                train_losses.update_state(t, pred)\n",
    "                train_accuracy.update_state(tf.argmax(t,1), tf.argmax(pred,1))\n",
    "            gradients = tape.gradient(loss, model.trainable_variables)\n",
    "            # set gradients to zero for all pruned weights\n",
    "            for gradient_matrix, variables in zip(gradients, model.trainable_variables):\n",
    "                gradient_matrix = np.where(variables == 0.0, 0.0, gradient_matrix)\n",
    "            optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "            \n",
    "        # test step\n",
    "        for x, t in test:\n",
    "            pred = model(x)\n",
    "            test_accuracy.update_state(tf.argmax(t,1), tf.argmax(pred,1))\n",
    "            test_losses.update_state(t, pred)\n",
    "        \n",
    "        train_acc.append(train_accuracy.result().numpy())\n",
    "        test_acc.append(test_accuracy.result().numpy())\n",
    "        train_l.append(train_losses.result().numpy())\n",
    "        test_l.append(test_losses.result().numpy())\n",
    "        train_accuracy.reset_state()\n",
    "        test_accuracy.reset_state()\n",
    "        train_losses.reset_state()\n",
    "        test_losses.reset_state()     \n",
    "    \n",
    "    return  train_acc, test_acc, train_l, test_l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4f6d2279",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the results\n",
    "\n",
    "def plot_losses(train_acc, test_acc, train_l, test_l, title):\n",
    "    fig= plt.figure(figsize=(10,6))\n",
    "    plt.title(title)\n",
    "    plt.xlabel(\"epochs\")\n",
    "    plt.ylabel(\"Loss/Accuracy\")\n",
    "    plt.plot(test_l,label=\"test loss\")\n",
    "    plt.plot(train_l,label=\"training loss\")\n",
    "    plt.plot(test_acc,label=\"test accuracy\")\n",
    "    plt.plot(train_acc,label=\"training accuracy\")\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "    max_test_acc_control = np.max(test_acc)\n",
    "    min_test_loss_control = np.min(test_l)\n",
    "    print(f\"lowest testing loss: {min_test_loss_control}\")\n",
    "    print(f\"highest testing accuracy: {max_test_acc_control}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f067558",
   "metadata": {},
   "source": [
    "## C: Putting it all togethter\n",
    "\n",
    "1. getting the initial weight configuration\n",
    "2. trianing the control model with the initial weights and storing the testing accuracy\n",
    "3. performing iterative magnitude pruning with resetting on the initial weight configuration and storing the resulting sparse network configuration\n",
    "4. train the resulting sparse network from the start and get the testing accuracy\n",
    "5. check if sparse network is a Winning Ticket by comparing accuracy with accurcy of control model\n",
    "6. do this multiple times to create a bunch of winning tickets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f6d9870e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to create sparse networks\n",
    "\n",
    "def create_sparse_net(train_dataset, test_dataset):\n",
    "    #########################################\n",
    "    # 1.:  get initial weight configuration #\n",
    "    #########################################\n",
    "\n",
    "    model = CNN2Model()\n",
    "\n",
    "    #predict something with the model so it initializes the weights\n",
    "    model(list(train_dataset)[0][0])\n",
    "\n",
    "    #save the initial weights to reuse for IMP\n",
    "    conv_initial_weights = []\n",
    "    for weight_matrix in model.get_conv_weights():\n",
    "        conv_initial_weights.append(weight_matrix)\n",
    "    dense_initial_weights = []\n",
    "    for weight_matrix in model.get_dense_weights():\n",
    "        dense_initial_weights.append(weight_matrix)\n",
    "        \n",
    "    #########################################################################\n",
    "    # 2.:  train control model with initial weights to get control accuracy #\n",
    "    #########################################################################\n",
    "\n",
    "    control_model = CNN2Model()\n",
    "    control_model(list(train_dataset)[0][0])\n",
    "    control_model.set_conv_weights(conv_initial_weights[0], conv_initial_weights[1], conv_initial_weights[2])\n",
    "    control_model.set_dense_weights(dense_initial_weights[0], dense_initial_weights[1])\n",
    "\n",
    "    train_acc, test_acc, train_l, test_l = train_loop(train_dataset, test_dataset, control_model, num_epochs=5)\n",
    "    control_stats = [train_acc, test_acc, train_l, test_l]\n",
    "    plot_losses(train_acc, test_acc, train_l, test_l,\"CNN Loss and Accuracy for control model\")\n",
    "    \n",
    "    #######################################################################################\n",
    "    # 3.:  use initial weights to also perform iterative magnitude pruning on a new model #\n",
    "    #######################################################################################\n",
    "\n",
    "    imp_model = CNN2Model()\n",
    "    imp_model(list(train_dataset)[0][0])\n",
    "    imp_model.set_conv_weights(conv_initial_weights[0], conv_initial_weights[1], conv_initial_weights[2])\n",
    "    imp_model.set_dense_weights(dense_initial_weights[0], dense_initial_weights[1])\n",
    "\n",
    "    train_acc, test_acc, train_l, test_l = train_loop_imp(train_dataset, test_dataset, imp_model)\n",
    "    imp_stats = [train_acc, test_acc, train_l, test_l]\n",
    "    plot_losses(train_acc, test_acc, train_l, test_l,\"CNN Loss and Accuracy for iterative magnitude pruning\")\n",
    "\n",
    "    #get resulting sparse network weights\n",
    "    conv_sparse_weights = []\n",
    "    for weight_matrix in imp_model.get_conv_weights():\n",
    "        conv_sparse_weights.append(weight_matrix)\n",
    "    dense_sparse_weights = []\n",
    "    for weight_matrix in imp_model.get_dense_weights():\n",
    "        dense_sparse_weights.append(weight_matrix)\n",
    "        \n",
    "    #############################################################################\n",
    "    # 4.:  train the resulting sparse network from the start and get accuracies #\n",
    "    #############################################################################\n",
    "\n",
    "    sparse_model = CNN2Model()\n",
    "    sparse_model(list(train_dataset)[0][0])\n",
    "    sparse_model.set_conv_weights(conv_sparse_weights[0], conv_sparse_weights[1], conv_sparse_weights[2])\n",
    "    sparse_model.set_dense_weights(dense_sparse_weights[0], dense_sparse_weights[1])\n",
    "\n",
    "    train_acc, test_acc, train_l, test_l = train_loop_sparse(train_dataset, test_dataset, sparse_model, num_epochs=5)\n",
    "    sparse_stats = [train_acc, test_acc, train_l, test_l]\n",
    "    plot_losses(train_acc, test_acc, train_l, test_l,\"CNN Loss and Accuracy for sparse network\")\n",
    "    \n",
    "    return control_stats, sparse_stats, conv_sparse_weights, dense_sparse_weights, imp_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46c016fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_lottery_tickets(runs):\n",
    "    \n",
    "    # get dataset\n",
    "    train_dataset, test_dataset = load_and_prep_cifar(batch_size=60, shuffle_size=512)\n",
    "    \n",
    "    for r in range(runs):\n",
    "        control_stats, sparse_stats, conv_sparse_weights, dense_sparse_weights, imp_model = create_sparse_net(train_dataset, test_dataset)\n",
    "        control_accuracy = np.max(control_stats[1])\n",
    "        sparse_accuracy = np.max(sparse_stats[1])\n",
    "        print(\"Best accuracy of control model: \", control_accuracy)\n",
    "        print(\"Best accuracy of sparse model: \", sparse_accuracy)\n",
    "        \n",
    "        # calculate percentage of pruned weights\n",
    "        all_weights = imp.model.get_weights()\n",
    "        pruned_amount = np.mean(all_weights.astype(bool).astype(int))\n",
    "        print(f\"The sparse model is pruned to {pruned_amount} of its original size\")\n",
    "        \n",
    "        if sparse_accuracy >= control_accuracy:\n",
    "            # safe winning ticket\n",
    "            print(\"It's a winning ticket!\")\n",
    "        else:\n",
    "            print(\"It's not a winning ticket.\")\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41268e7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "create_lottery_tickets(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f52039ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# code block for trying out to safe weights in a file\n",
    "\n",
    "model = CNN2Model()\n",
    "model(list(train_dataset)[0][0])\n",
    "#model.save_weights(\"...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58bbad3e",
   "metadata": {},
   "source": [
    "## D: create multiple Winning Tickets and store their weight configuration in a file"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
